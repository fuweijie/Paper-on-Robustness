# Representative papers on adversarial robustness.
###### Number of :star: provides a suggested order for paper reading.
##	Review:
######	:star::star::star::star::star: Adversarial attacks and defenses in images, graphs and text: a review, 2019.
##	Attackers (small pertumbation can mislead classification and explanation):
######	:star::star::star::star: Explaining and harnessing adversarial examples, 2015, ICLR.
######	:star::star: Deepfool: a simple and accurate method to fool deep neural networks, 2016,CVPR.
######	:star::star: Zoo: zeroth order optimization based black-box attacks to deep neural networks without training substitute models, 2017.
######	:star::star: The limitations of deep learning in adversarial settings, 2018.
######	:star::star::star: Interpretable deep learning under fire, 2018.
######	:star::star::star: Interpretation of neural networks is fragile, 2019, AAAI.
##	Analysis and insights (the reason of adversarial examples):
######	:star::star: Perturbation, optimization and statistics, 2016.
######	:star::star: Sparse dnns with improved adversarial robustness, 2018, NeurIPS.
######	:star::star: Attention, please! Adversarial defense via attention rectification and preservation, 2019.
######	:star::star: Defective convolutional layers learn robust cnns, 2019.
######	:star::star::star::star: Adversarial examples are not bugs, they are features, 2019, NeurIPS.
######	:star::star: Batch normalization is a cause of adversarial vulnerability, 2019, ICML.
######	:star::star::star: Adversarial training can hurt generalization, 2019.
######	:star::star::star: Proper network interpretability helps adversarial robustness in classification, 2020, ICML.
######	:star::star::star: Exploring the vulnerability of deep neural networks: a study of parameter corruption, 2020.
######	:star::star::star::star: Overfitting in adversarially robust deep learning, 2020, ICML.
######	:star::star::star::star: The curious case of adversarially robust models: more data can help, double descend, or hurt generalization, 2020.
######	:star::star: Rethinking randomized smoothing for adversarial robustness, 2020.
######	:star::star::star: Invariance vs. Robustness of neural networks, 2020.
##	About fake defense (these defense we can't trust):
######	:star::star::star::star: Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples, 2018.
##	Defensing via adversarial training (using data augumentation):
######	:star::star: Adversarial machine learning at scale, 2017, ICLR.
######	:star: Mixup inference: better exploiting mixup to defend adversarial attacks, 2019.
######	:star::star: Towards a unified min-max framework for adversarial exploration and robustness, 2019.
######	:star::star::star: Theoretically principled trade-off between robustness and accuracy, 2019.
######	:star::star: L1-norm double backpropagation adversarial defense, 2019.
######	:star: Learning to defense by learning to attack, 2019.
######	:star: Adversarial vertex mixup: toward better adversarially robust generalization, 2020.
######	:star::star: Manifold regularization for adversarial robustness, 2020.
######	:star: Curriculum adversarial training, 2020.
##	Defensing via denoising (modifying networks):
######	:star: Deepcloak: masking deep neural network models for robustness against adversarial samples, 2017.
######	:star::star: Adversarial examples detection in deep networks with convolutional filter statistics, 2017, ICCV.
######	:star: Optimal transport classifier: defending against adversarial attacks by regularized deep embedding, 2018.
######	:star::star::star: Feature denoising for improving adversarial robustness, 2019, CVPR.
######	:star::star: Adversarial noise layer: regularize neural network by adding noise, 2019, ICIP.
######	:star::star: Feature losses for adversarial robustness, 2019.
######	:star: White-box adversarial defense via self-supervised data estimation, 2019.
######	:star: Adversarial defense by stratified convolutional sparse coding, 2019.
##	Defensing via regularization (using better regularization terms):
######	:star::star::star::star: Adversarial robustness through local linearization, 2019, NeurIPS.
######	:star: Adversarial defense by restricting the hidden space of deep neural networks, 2019.
######	:star::star: Jacobian adversarially regularized networks for robustness, 2020.
######	:star::star::star: Towards understanding the regularization of adversarial robustness on neural networks, 2020, ICML.
##	Theory on error bounds (quantitative analysis on robustness):
######	:star: Towards deep neural network architectures robust to adversarial examples, 2014.
######	:star::star: Limitations of the lipschitz constant as a defense against adversarial examples, 2018.
######	:star: Towards robust neural networks via random self-ensemble, 2018, ECCV.
######	:star::star: Lipschitz-margin training: scalable certification of perturbation invariance for deep neural networks, 2018, NeurIPS.
######	:star::star: Lipschitz regularity of deep neural networks: analysis and efficient estimation, 2018, NeurIPS.
######	:star::star::star: Efficient and accurate estimation of lipschitz constants for deep neural networks, 2019, NeurIPS.
######	:star::star: Adversarial learning guarantees for linear hypotheses and neural networks, 2020, ICML.
## Further Reading
https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html


